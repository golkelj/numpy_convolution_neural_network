{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b964fab8",
   "metadata": {},
   "source": [
    "## Convolution Neural Network built fully mainly with linear alegbra and chain rule of calculus\n",
    "\n",
    "* Note that tensorflow and scipy are only used to load the dataset in, and for the correlate math function simply for convience of this project, It does not functional take away from the object of this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy.signal as signal \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de3d4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data in\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# normalize\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "# reshape to (n, 1, 28, 28) for cnn input\n",
    "x_train_cnn = x_train.reshape(-1, 1, 28, 28)\n",
    "x_test_cnn = x_test.reshape(-1, 1, 28, 28)\n",
    "\n",
    "# one-hot encode the targets\n",
    "y_train_oh = np.eye(10)[y_train].reshape(-1, 10, 1)\n",
    "y_test_oh = np.eye(10)[y_test].reshape(-1, 10, 1)\n",
    "\n",
    "number_training = x_train_cnn.shape[0]\n",
    "\n",
    "shuffled_indices = np.random.permutation(number_training)\n",
    "\n",
    "# shuffle\n",
    "x_train_cnn = x_train_cnn[shuffled_indices]\n",
    "y_train_oh= y_train_oh[shuffled_indices]\n",
    "\n",
    "# checks\n",
    "assert x_train_cnn.shape[0] == y_train_oh.shape[0], \"Number of training samples must match number of labels. Error in loading data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bfb54",
   "metadata": {},
   "source": [
    "## A visualization of the images that we are working with are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e4e2ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582482b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: return output\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: update weights and return input gradient\n",
    "        pass\n",
    "\n",
    "class convolution(layer):\n",
    "    \"\"\"\n",
    "    The depth is number of kernals \n",
    "    \"\"\" \n",
    "    def __init__(self, kernel_size : int, input_shape : tuple , depth : int): \n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)  * 0.01\n",
    "        self.biases = np.random.randn(*self.output_shape) * 0.01\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\")\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        \"\"\" \n",
    "        Kernal gradient is ∂E/∂K = Xj * ∂E/∂Y\n",
    "        Biases gradient is ∂E/∂B = ∂E/∂Y - This is given by the output gradient, this will be the gradient of the loss funtion\n",
    "        Input gradient is ∂E/∂X = ∑(∂E/∂Y *(full) K)\n",
    "        \"\"\"\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                kernels_gradient[i, j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
    "                input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i, j], \"full\")\n",
    "\n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "        return input_gradient\n",
    "    \n",
    "class reshape(layer): \n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.reshape(input ,self.output_shape)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)\n",
    "\n",
    "\n",
    "class dense(layer): \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.biases = np.zeros((output_size, 1))  \n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.weights.T, input) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        input_gradient = np.dot(self.weights, output_gradient)\n",
    "        weights_gradient = np.dot(self.input, output_gradient.T)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "        return input_gradient  \n",
    "       \n",
    "       \n",
    "class activation(layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
    "\n",
    "class relu(activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.maximum(0, x)\n",
    "        def relu_prime(x):\n",
    "            return (x > 0).astype(float)\n",
    "        super().__init__(relu, relu_prime)\n",
    "        \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x, axis=0, keepdims=True)\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred + 1e-8))\n",
    "\n",
    "def categorical_cross_entropy_prime(y_true, y_pred):\n",
    "    return y_pred - y_true\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    convolution(kernel_size=3, input_shape=(1, 28, 28), depth=5),\n",
    "    relu(),\n",
    "    reshape(input_shape=(5, 26, 26), output_shape=(5 * 26 * 26, 1)),\n",
    "    dense(5 * 26 * 26, 100),\n",
    "    relu(),\n",
    "    dense(100, 10)          \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c808db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, input):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    output = softmax(output) \n",
    "    return output\n",
    "\n",
    "def train(network, loss, loss_prime, x_train, y_train, epochs = 1000, learning_rate = 0.01, verbose = True):\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            output = predict(network, x)\n",
    "\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0561b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20, error=0.6585352059274021\n",
      "2/20, error=0.4257140693930467\n",
      "3/20, error=0.34903301016377347\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5000\n",
    "\n",
    "x_train_cnn_small = x_train_cnn[:n_samples]\n",
    "y_train_oh_small = y_train_oh[:n_samples]\n",
    "x_test_cnn_small = x_test_cnn[:n_samples]\n",
    "y_test_oh_small = y_test_oh[:n_samples]\n",
    "\n",
    "# train\n",
    "train(\n",
    "    network,\n",
    "    categorical_cross_entropy,\n",
    "    categorical_cross_entropy_prime,\n",
    "    x_train_cnn_small,\n",
    "    y_train_oh_small,\n",
    "    epochs=20, # fewer epochs for speed, and cause I am on a cpu \n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# test\n",
    "correct = 0\n",
    "total = len(x_test_cnn_small)\n",
    "for x, y in zip(x_test_cnn_small, y_test_oh_small):\n",
    "    output = predict(network, x)\n",
    "    pred = np.argmax(output)\n",
    "    true = np.argmax(y)\n",
    "    if pred == true:\n",
    "        correct += 1\n",
    "    print(f\"pred: {pred}, true: {true}\")\n",
    "print(f\"Accuracy: {correct/total:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
